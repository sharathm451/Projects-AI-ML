{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.metrics import Accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "import imutils\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_datagen = ImageDataGenerator(rotation_range=40,\n",
    "#                                   width_shift_range=0.2,\n",
    "#                                  height_shift_range=0.2,\n",
    "#                                   rescale=1.0/255,\n",
    "#                                   shear_range=0.2,\n",
    "#                                   zoom_range=0.2,\n",
    "#                                   horizontal_flip=True,\n",
    "#                                   fill_mode='nearest')\n",
    "\n",
    "#because already we have taken another dataset which is augmented so need all this features\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the train directory images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# from PIL import Image\n",
    "# import glob\n",
    "\n",
    "# image_list = []\n",
    "# for filename in glob.glob(r'E:\\AI-ML PYTHON\\Project-7\\Mask dataset\\dest_folder\\train/*.jpg'): #assuming gif\n",
    "#     im=Image.open(filename)\n",
    "#     image_list.append(im)\n",
    "    \n",
    "# result = ''.join([s.encode('ascii','ignore') for s in image_list])\n",
    "\n",
    "# # This portion is part of my test code\n",
    "# byteImgIO = io.BytesIO()\n",
    "# byteImg = Image.open(result)\n",
    "# byteImg.save(byteImgIO, \"PNG\")\n",
    "# byteImgIO.seek(0)\n",
    "# byteImg = byteImgIO.read()\n",
    "\n",
    "\n",
    "# # Non test code\n",
    "# dataBytesIO = io.BytesIO(byteImg)\n",
    "# Image.open(dataBytesIO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(r\"E:\\AI-ML PYTHON\\Project-7\\Mask dataset\\data\\train\",\n",
    "                                               target_size=(100,100),\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the test directory images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = test_datagen.flow_from_directory(r\"E:\\AI-ML PYTHON\\Project-7\\Mask dataset\\data\\test\",\n",
    "                                            target_size = (100,100),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = Input(shape=(100,100,3))\n",
    "\n",
    "# x = Conv2D(100, (3, 3), activation='relu', padding='same')(i)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Conv2D(100, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D((2, 2))(x)\n",
    "# # x = Dropout(0.2)(x)\n",
    "# x = Conv2D(200, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Conv2D(200, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D((2, 2))(x)\n",
    "# # x = Dropout(0.2)(x)\n",
    "# x = Conv2D(300, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Conv2D(300, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D((2, 2))(x)\n",
    "# # x = Dropout(0.2)(x)\n",
    "\n",
    "# # x = GlobalMaxPooling2D()(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# model = Model(i,x)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(200,(3,3),input_shape=(100,100,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The first CNN layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Conv2D(100,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "#Flatten layer to stack the output convolutions from second convolution layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "#Dense layer of 64 neurons\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "#The Final layer with two outputs for two categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',\n",
    "                             verbose=0,save_best_only=True,mode='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.3013 - accuracy: 0.4556WARNING:tensorflow:From D:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From D:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model2-001.model\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.3013 - accuracy: 0.4556 - val_loss: 0.6952 - val_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.4222INFO:tensorflow:Assets written to: model2-002.model\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6940 - accuracy: 0.4222 - val_loss: 0.6932 - val_accuracy: 0.4667\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5111INFO:tensorflow:Assets written to: model2-003.model\\assets\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6932 - accuracy: 0.5111 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5222INFO:tensorflow:Assets written to: model2-004.model\\assets\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6931 - accuracy: 0.5222 - val_loss: 0.6931 - val_accuracy: 0.5333\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 4s 1s/step - loss: 0.6931 - accuracy: 0.4000 - val_loss: 0.6931 - val_accuracy: 0.4667\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.4444 - val_loss: 0.6931 - val_accuracy: 0.4500\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.4444INFO:tensorflow:Assets written to: model2-007.model\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6931 - accuracy: 0.4444 - val_loss: 0.6931 - val_accuracy: 0.4833\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.4444 - val_loss: 0.6931 - val_accuracy: 0.5333\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.4000 - val_loss: 0.6931 - val_accuracy: 0.4667\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.4889 - val_loss: 0.6931 - val_accuracy: 0.4000\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.3837 - val_loss: 0.6931 - val_accuracy: 0.4833\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5111INFO:tensorflow:Assets written to: model2-012.model\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6931 - accuracy: 0.5111 - val_loss: 0.6931 - val_accuracy: 0.3500\n",
      "Epoch 13/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.4556 - val_loss: 0.6931 - val_accuracy: 0.4667\n",
      "Epoch 14/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.3667 - val_loss: 0.6931 - val_accuracy: 0.4333\n",
      "Epoch 15/15\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6931 - accuracy: 0.4111 - val_loss: 0.6931 - val_accuracy: 0.3833\n"
     ]
    }
   ],
   "source": [
    "histroy=model.fit( train_data,steps_per_epoch=100//batch_size,epochs=15,\n",
    "                            callbacks=callbacks_list,\n",
    "                             validation_data=test_data,validation_steps=70//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mask_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('mask_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr = cv2.CascadeClassifier(r\"E:\\AI-ML PYTHON\\Project-7\\Mask dataset/haarcascade_frontalface_default - Govindaraj V.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={0:'Mask',1:'No mask'}\n",
    "GR_dict={0:(0,255,0),1:(0,0,255)}\n",
    "\n",
    "rect_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    (ret,img) = capture.read()\n",
    "    img = cv2.flip(img,1,1)\n",
    "    \n",
    "    rerect_size = cv2.resize(img, (img.shape[1] // rect_size, img.shape[0] // rect_size))\n",
    "    faces = face_clsfr.detectMultiScale(rerect_size)\n",
    "    \n",
    "    for f in faces:\n",
    "        (x,y,w,h) = [i*rect_size for i in f]\n",
    "        \n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        rerect_size = cv2.resize(face_img,(100,100))\n",
    "        normalized=rerect_size/255.0\n",
    "        reshaped=np.reshape(normalized,(1,100,100,3))\n",
    "        reshaped=np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        cv2.rectangle(img,(x,y), (x+w, y+h),GR_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),GR_dict[label],-1)\n",
    "        cv2.putText(img, results[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "\n",
    "        \n",
    "\n",
    "    cv2.imshow('result',img)\n",
    "    if cv2.waitKey(2)==27:\n",
    "            break\n",
    "capture.release() \n",
    "cv2.destroyAllWindows()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source=cv2.VideoCapture(0)\n",
    "\n",
    "# labels_dict={0:'MASK',1:'NO MASK'}\n",
    "# color_dict={0:(0,255,0),1:(0,0,255)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while(True):\n",
    "\n",
    "#     ret,img=source.read()\n",
    "# #    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "#     faces=face_clsfr.detectMultiScale(img,1.3,5)  \n",
    "\n",
    "#     for x,y,w,h in faces:\n",
    "    \n",
    "#         face_img=img[y:y+w,x:x+w]\n",
    "#         resized=cv2.resize(face_img,(100,100))\n",
    "#         normalized=resized/255.0\n",
    "#         reshaped=np.reshape(normalized,(1,100,100,3))\n",
    "#         result=model.predict(reshaped)\n",
    "\n",
    "#         label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "#         cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "#         cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "#         cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "        \n",
    "#     cv2.imshow('LIVE',img)\n",
    "#     key=cv2.waitKey(1)\n",
    "    \n",
    "#     if(key==27):\n",
    "#         break\n",
    "        \n",
    "# cv2.destroyAllWindows()\n",
    "# source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
